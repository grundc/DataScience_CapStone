---
title: "Data Science Capstone Project Week2 Exporation"
author: "by Christoph"
date: "18 Februar 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, messages=FALSE, warning = FALSE)
library(dplyr)
library(stringi)
library(ggplot2)
library(tidytext)
library(tidyr)
library(igraph)
library(intergraph)
library(GGally)
```
## Introduction

The following document gives a brief exploratory analysis about the source files provided to train a model for word prediction.
The later solution, which will use the "Markov Chain" approach to predict words, given another word.


## The source files

The input data for the later text prediction project consists of 3 files. One from Twitter, one from a blog feed and on from a news feed.
Some basic properties for these files are listed below.
```{r }
filenames <- c("en_US.news.txt","en_US.twitter.txt","en_US.blogs.txt")
filelength <- integer()
filesize <- integer()
maxentry <- integer()
saveFile <- "fileoverview.rda"

if (!file.exists(saveFile))
{
      
      for(filename in filenames)
      {
            con <- file(paste("../data/",filename, sep=""), "r") 
            complete_file <- readLines(con, encoding="UTF-8")
            
            amount_rows <- length(complete_file)
            filelength <- c(filelength,amount_rows )
            size <- as.integer(object.size(complete_file) / (1024*1024)) # calcualted in MB
            filesize <- c(filesize, size)
            maxChar <- max(stri_length(complete_file))
            maxentry <- c(maxentry, maxChar)
            close(con)
      }
      
      fileoverview <- data.frame(filenames, filesize, filelength, maxentry)
      colnames(fileoverview)<- c("FILE","OBJECT SIZE/MB", "ROWS", "MAX TEXT SIZE")
      
      
      save(fileoverview, file = saveFile) 
      rm(fileoverview)
      rm(complete_file)
      
}

load(saveFile)
fileoverview
```
  
**FILES:**        source file name  
**OBJECT SIZE:**  size in MB when loaded into memory  
**ROWS:**         amount entries  
**MAX TEXT:**     The entry with the most characters   


## Exploring the files in detail

As the files are quite big and some processing requires a lot of system resources, the following analysis was done against a sample set of these 3 files, randomly gernerated. Sample size per file = 10% of the orginal file.
```{r }
strFilename <- "fileSampleDF.rda"
set.seed(1001)
all_file_df <- data.frame()
  
if (!file.exists(strFilename))
  {
    
    for(filename in filenames)
    {
      con <- file(paste("../data/",filename, sep=""), "r") 
      complete_file <- readLines(con, encoding="UTF-8")
      Integrated <- rbinom(length(complete_file),1, prob=0.1)
      file_df <- data.frame(line = 1:length(complete_file), text = complete_file, stringsAsFactors = F, Integrated=Integrated, file = filename)
      file_df <- file_df %>% filter(Integrated==1)
      all_file_df <- rbind(all_file_df,file_df)
      rm(complete_file)
      close(con)
    }
    
   
    save(all_file_df, file=strFilename)
    rm(file_df)
    rm(all_file_df)
  }
  

load(strFilename)
```

**FINDING:** As we can see, there shortcuts like "lol" (laugh -out-laut) or "rt" in the ranking. These are not really word and not suprinsingly we can fin them in the twitter feed. We have to find a way to deal with them.  

### 1.) Simple tokenization

First we simply cut the texts into single words, also called tokenization. So we can do some basic statistical operations like counting words. Addionally stop words (e.g. "the") were excluded, as these don't give meaning to a text and  would appear at the top of any ranking.

The histogram shows the 10 most used words per each of the 3 files. 
```{r }
strFilename <- "all_files_words.rda"
if (!file.exists(strFilename))
{
  all_files_words <- all_file_df %>% unnest_tokens(word, text, token="words") %>% anti_join(stop_words) %>% count(file,word)
  save(all_files_words, file=strFilename)
}

load(strFilename)

# Plotting
all_files_words  %>% group_by(file) %>% top_n(n=10, wt=n) %>%
  ggplot(aes(word,n, group=factor(file))) +
  geom_bar(stat="identity") + 
  xlab(NULL) + coord_flip() + facet_grid(. ~ file) 

rm(all_files_words)
```


### 2.) Analyzing n-grams

N-grams are combination of words. The next analysis looks at 2-grams, which means basically 2 consecutive words. This is already a good step in the direction of prediction, as it will give us a hint, which could be a reasonable next word following a given word.
As with simple word tokenization we have to exclude word combinations with stop words.

The following grafic shows the top 10 most used 2-grams per each of our 3 files:

```{r }

strFilename <- "all_files_bigrams.rda"
if (!file.exists(strFilename))
{
  all_files_Bigrams <- all_file_df %>% unnest_tokens(bigram, text, token="ngrams" , n=2)

  all_files_Bigrams <- all_files_Bigrams %>% separate(bigram, c("word1", "word2"), sep=" ") %>%  # separate into words
                                    filter(!word1 %in% stop_words$word) %>%                 # filter out where any common stop word
                                    filter(!word2 %in% stop_words$word) %>%
                                    unite(bigram, word1, word2, sep=" ")
                          
  save(all_files_Bigrams, file=strFilename)
}

load(strFilename)

all_files_Bigrams %>% count(file,bigram) %>% group_by(file) %>% top_n(n=10, wt=n) %>%
  ggplot(aes(bigram,n, group=factor(file))) +
  geom_bar(stat="identity") + 
  xlab(NULL) + coord_flip() + facet_grid(. ~ file) 

#rm(all_files_Bigrams)

```

**FINDING:** Also here we see word repetitions like "rt rt" high in the ranking. Also we can see e.g. for "mother's day" and "mothers day", that even if they mean the same, they are conisdered as different combinations because of a different (wrong) writing.

An n-gram analysis can also be visualized as a network, which shows graphically, which word follows another word. In fact this is the visualization of a "Markov chain"." Only combinations with 300 repetitions were considered, otherwise it would look like a big mess:

```{r }

Bigramgraph <- all_files_Bigrams %>% count(bigram) %>% filter(n > 300) %>%
                        separate(bigram, c("word1", "word2"), sep=" ") %>%
                        graph_from_data_frame()                         # Genrates an igraph
plot(Bigramgraph)
# plot(blogBigramgraph, vertex.size=4, edge.arrow.size=0.15, edge.arrow.width=2, edge.color="black")
rm(all_files_Bigrams)
```

### 3.) "Term frequency" and "Inverse document frequency"

Term frequency gives an impression on word importance in relation to the amount of total words in a text. Additionally "inverse document frequency (idf)" is a technic to up-weight words that appear less frequent over the texts:
```{r }
# filenames <- c("en_US.news.txt","en_US.twitter.txt","en_US.blogs.txt")
PerEntryWords <- all_file_df %>% mutate(entry = paste(file,line, sep="_")) %>%
  unnest_tokens(word, text, token="words") %>%          
  count(entry, word) %>% ungroup()            

PerEntryWords %>% filter(grepl("en_US.news.txt", entry)) %>% bind_tf_idf(word,entry, n) %>% 
filter(tf != 1) %>% arrange(desc(tf_idf)) %>% select(entry,word, n, tf,idf,tf_idf) %>% top_n(10)


```

```{r }
PerEntryWords %>% filter(grepl("en_US.twitter.txt", entry)) %>% bind_tf_idf(word,entry, n) %>% 
filter(tf != 1) %>% arrange(desc(tf_idf)) %>% select(entry,word, n, tf,idf,tf_idf) %>% top_n(10)
```


```{r }
PerEntryWords %>% filter(grepl("en_US.blogs.txt", entry)) %>% bind_tf_idf(word,entry, n) %>% 
filter(tf != 1) %>% arrange(desc(tf_idf)) %>% select(entry,word, n, tf,idf,tf_idf) %>% top_n(10)
```

Even if this method might not help use that much in terms of predictions, we see a lot of words, which are not really words or have not really a meaning, but haevily used in today's chats to express emotiones, opinions etc. 

## Summary

From this first exploration it is obvious, that just removing so called "stop words" and running an n-gram tokenization to create a precition model with a "Markov Chain" is not sufficient. We have to validate words somehow to ensure we deal with real words and not fantacy words. Also different writings of the same phrase will be a challenge. Also we could see that after a number very often we have time measurement like minutes e.g. May be we should conisder a rule, that if a number is typed by a user, we offer typical tie measurements like "minutes", "month", "year"....