---
title: "Data Science Capstone Project"
author: "by Christoph"
date: "6 Februar 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, messages=FALSE, warning = FALSE)
library(dplyr)
library(stringi)
library(ggplot2)
library(tidytext)
library(tidyr)
library(igraph)
library(intergraph)
library(GGally)
```

## The data to be used


### The source files
```{r }
filenames <- c("en_US.news.txt","en_US.twitter.txt","en_US.blogs.txt")
filelength <- integer()
filesize <- integer()
maxentry <- integer()
saveFile <- "fileoverview.rda"

if (!file.exists(saveFile))
{
      
      for(filename in filenames)
      {
            con <- file(paste("../data/",filename, sep=""), "r") 
            complete_file <- readLines(con)
            
            amount_rows <- length(complete_file)
            filelength <- c(filelength,amount_rows )
            size <- as.integer(object.size(complete_file) / (1024*1024)) # calcualted in MB
            filesize <- c(filesize, size)
            maxChar <- max(stri_length(complete_file))
            maxentry <- c(maxentry, maxChar)
            close(con)
      }
      
      fileoverview <- data.frame(filenames, filesize, filelength, maxentry)
      colnames(fileoverview)<- c("FILE","SIZE/MB", "LENGTH", "MAXTEXT")
      
      
      save(fileoverview, file = saveFile) 
      rm(fileoverview)
      rm(complete_file)
      
}

load(saveFile)
fileoverview
```

## Exploring the US blog file



### 1.) Simple tokenization

```{r }
fileBlogSample <- "blogfiledf.rda"
set.seed(1001)
if (!file.exists(fileBlogSample))
{
      con <- file("../data/en_US.blogs.txt", "r") 
      blog_file <- readLines(con, encoding="UTF-8")
      close(con)
      Integrated <- rbinom(length(blog_file),1, prob=0.1)
      blog_file_df <- data.frame(line = 1:length(blog_file), text = blog_file, stringsAsFactors = F, Integrated=Integrated)
      blog_file_df <- blog_file_df %>% filter(Integrated==1)
      rm(blog_file)
      save(blog_file_df, file=fileBlogSample)
      rm(blog_file_df)
}
load(fileBlogSample)
```

This figure shows the top most used words in the US-Blog dataset. So called stop-words were excluded to get a clearer picture on the words that give content to the blogs.
```{r }

blog_words <- blog_file_df %>% unnest_tokens(word, text, token="words")

blog_words %>% anti_join(stop_words) %>% count(word, sort=TRUE) %>% top_n(20) %>%
  ggplot(aes(word,n)) +
  geom_bar(stat="identity") +
  xlab(NULL) + coord_flip()
```


### 2.) "Term frequency" and "Inverse document frequency"

```{r }
perBlogWords <- blog_file_df %>% mutate(blog = paste("blog",line, sep="")) %>% 
  unnest_tokens(word, text, token="words") %>%          # tokenization
  count(blog, word, sort=TRUE) %>% ungroup()            # counting words per blog entry


total_blog_words <- perBlogWords %>% group_by(blog) %>% summarize(total=sum(n))                # counting total words per blog entry

summary_words <- left_join(perBlogWords, total_blog_words)

rm(perBlogWords)
rm(total_blog_words)

summary_words <- summary_words %>% arrange(desc(n)) %>% group_by(blog) %>% mutate(rank=row_number(), term_freq = n/total)

summary_words <- summary_words %>% filter(term_freq != 1)  

summary_words <- summary_words %>% bind_tf_idf(word,blog, n)

summary_words %>% arrange(desc(tf_idf)) %>% select(blog,word, n, total,tf,idf,tf_idf) %>% top_n(20)
rm(summary_words)
```


### 3.) Analyzing n-grams

```{r }

blogBigrams <- blog_file_df %>% mutate(blog = paste("blog",line, sep="")) %>% 
      unnest_tokens(bigram, text, token="ngrams" , n=2)


blogBigrams <- blogBigrams %>% separate(bigram, c("word1", "word2"), sep=" ") %>%  # separate into words
                                    filter(!word1 %in% stop_words$word) %>%                 # filter out where any common stop word
                                    filter(!word2 %in% stop_words$word) %>%
                                    unite(bigram, word1, word2, sep=" ") %>%                # recombine words
                                    count(bigram, sort=TRUE)

blogBigrams <- blogBigrams %>% filter(n > 4)
blogBigrams %>% top_n(20)

```



```{r }
blogBigramgraph <- blogBigrams %>% filter(n > 500) %>% 
                        separate(bigram, c("word1", "word2"), sep=" ") %>% 
                        graph_from_data_frame()                         # Genrates an igraph
plot(blogBigramgraph, vertex.size=4, edge.arrow.size=0.15, edge.arrow.width=2, edge.color="black")

```